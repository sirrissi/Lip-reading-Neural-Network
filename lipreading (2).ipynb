{"cells":[{"cell_type":"markdown","metadata":{},"source":["load the data/\n","SKIP to 'np.load('newX6.npy')' to avoid having to run the pre processing code "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:17:21.827963Z","iopub.status.busy":"2023-12-05T14:17:21.827588Z","iopub.status.idle":"2023-12-05T14:17:21.834133Z","shell.execute_reply":"2023-12-05T14:17:21.833013Z","shell.execute_reply.started":"2023-12-05T14:17:21.827932Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","from sklearn.preprocessing import MinMaxScaler,StandardScaler\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-05T14:15:39.989036Z","iopub.status.busy":"2023-12-05T14:15:39.988633Z","iopub.status.idle":"2023-12-05T14:16:32.369397Z","shell.execute_reply":"2023-12-05T14:16:32.367965Z","shell.execute_reply.started":"2023-12-05T14:15:39.989004Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","dataframes_list = []\n","directory_path = r'/train10/train10'\n","\n","for subdir in os.listdir(directory_path):\n","    subdir_path = os.path.join(directory_path, subdir)\n","\n","    if os.path.isdir(subdir_path):  \n","        word_dataframes = []\n","\n","        for file in os.listdir(subdir_path):\n","            file_path = os.path.join(subdir_path, file)\n","            df = pd.read_csv(file_path)\n","            word_dataframes.append(df)\n","\n","        dataframes_list.append(word_dataframes) # List of list of dataframes, dataframes_list[1] is the list of all data from the csv files in the addition directory\n"]},{"cell_type":"markdown","metadata":{},"source":["Some useful functions for the preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:14:05.524213Z","iopub.status.busy":"2023-12-05T14:14:05.522967Z","iopub.status.idle":"2023-12-05T14:14:05.828028Z","shell.execute_reply":"2023-12-05T14:14:05.827064Z","shell.execute_reply.started":"2023-12-05T14:14:05.524140Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def nb_changes(polarity_list):\n","    changes = np.diff(polarity_list)\n","    return np.sum(changes != 0)\n","\n","\n","import cv2\n","import numpy as np\n","def downsize(factor, tensor) : \n","\n","    resized_frames = np.empty((tensor.shape[0], 30, int(tensor.shape[2] * factor), int(tensor.shape[3] * factor),tensor.shape[4]))\n","\n","    for i in range(tensor.shape[0]):\n","        for j in range(tensor.shape[1]):\n","            for k in range(tensor.shape[4]):\n","\n","                resized_frame = cv2.resize(tensor[i, j, :, :,k], (0, 0), fx=factor, fy=factor)\n","\n","                resized_frames[i, j, :, :,k] = resized_frame\n","    return resized_frames\n"]},{"cell_type":"markdown","metadata":{},"source":["pre process the data by splitting into frames and extracting features :\n","number of changes, sum of polarities, positive changes and length of the list of polarities captured in a timeframe for a given pixel.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:40:27.910844Z","iopub.status.busy":"2023-12-05T14:40:27.910408Z","iopub.status.idle":"2023-12-05T14:54:38.316067Z","shell.execute_reply":"2023-12-05T14:54:38.315154Z","shell.execute_reply.started":"2023-12-05T14:40:27.910812Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["x_range = range(50, 170)\n","y_range = range(15, 65)\n","DF2 = pd.DataFrame([(x, y) for x in x_range for y in y_range], columns=['x', 'y'])\n","all_results = []\n","num_frames=30\n","scaler=StandardScaler()\n","for list_of_df in dataframes_list:\n","    list_results = []  # List of results for each time interval\n","\n","    # Loop over each dataframe in the inner list\n","    for df in list_of_df:\n","        results = []\n","        med=df.loc[:,'time'].median()\n","        if med <600000 :\n","            interval_to_slice=df[(df['time'] < 1200000)]\n","        elif med >2400000 :\n","            interval_to_slice=df[(df['time'])>1800000]\n","        else :\n","            interval_to_slice=df[(df['time']>med-600000) & (df['time']<med+600000)]\n","        \n","        \n","        time_intervals = np.linspace(interval_to_slice['time'].to_numpy()[0], interval_to_slice['time'].to_numpy()[len(interval_to_slice)-1], num=num_frames + 1)\n","        for i in range(len(time_intervals) - 1):    \n","            start_time, end_time = time_intervals[i], time_intervals[i + 1]\n","            interval_data = interval_to_slice[(interval_to_slice['time'] >= start_time) & (interval_to_slice['time'] < end_time)]\n","            interval_data=interval_data[ (interval_data['x']>=50) & (interval_data['x']<=170) & (interval_data['y']>=15) & (interval_data['y']<=65) ]\n","            last_polarity_per_location =interval_data.groupby(['x', 'y'])['polarity'].sum().reset_index()\n","            last_pol_merged=pd.merge(DF2, last_polarity_per_location, on=['x', 'y'], how='left').fillna(0)\n","\n","            count_polarity_per_location =interval_data.groupby(['x', 'y'])['polarity'].count().reset_index()\n","            count_pol_merged=pd.merge(DF2, count_polarity_per_location, on=['x', 'y'], how='left').fillna(0)\n","\n","            grouped_data = interval_data.groupby(['x', 'y']).agg({'polarity': nb_changes}).reset_index()\n","            DF_merged = pd.merge(DF2, grouped_data, on=['x', 'y'], how='left')\n","            DF_merged['polarity'].fillna(0, inplace=True)\n","\n","            matrix1=last_pol_merged.pivot(index='y', columns='x', values='polarity').values\n","            matrix2=count_pol_merged.pivot(index='y', columns='x', values='polarity').values\n","            matrix3=DF_merged.pivot(index='y', columns='x', values='polarity').values\n","            \n","\n","            matrix3D = np.dstack((matrix1,matrix2,matrix3))\n","            results.append(matrix3D)\n","        results=np.array(results)\n","        list_results.append(results)\n","    all_results.append(np.array(list_results))\n","    print('one directory done')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:54:43.435905Z","iopub.status.busy":"2023-12-05T14:54:43.435422Z","iopub.status.idle":"2023-12-05T14:54:45.002385Z","shell.execute_reply":"2023-12-05T14:54:45.001326Z","shell.execute_reply.started":"2023-12-05T14:54:43.435874Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["X = np.array(all_results)\n","X1 = X.reshape(320,30,50,120,3)\n","np.save('newX6resc.npy',X1) "]},{"cell_type":"markdown","metadata":{},"source":["Load the previously preprocessed data to avoid having to run the code again"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:13:21.584158Z","iopub.status.busy":"2023-12-05T19:13:21.583611Z","iopub.status.idle":"2023-12-05T19:13:27.258742Z","shell.execute_reply":"2023-12-05T19:13:27.257303Z","shell.execute_reply.started":"2023-12-05T19:13:21.584118Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","X1=np.load('newX6resc.npy').reshape(320,30,50,120,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:54:48.643289Z","iopub.status.busy":"2023-12-05T14:54:48.642890Z","iopub.status.idle":"2023-12-05T14:54:48.651894Z","shell.execute_reply":"2023-12-05T14:54:48.650378Z","shell.execute_reply.started":"2023-12-05T14:54:48.643258Z"},"trusted":true},"outputs":[],"source":["def downsize(factor, tensor) : \n","\n","    resized_frames = np.empty((tensor.shape[0], 30, int(tensor.shape[2] * factor), int(tensor.shape[3] * factor),tensor.shape[4]))\n","\n","    for i in range(tensor.shape[0]):\n","        for j in range(tensor.shape[1]):\n","            for k in range(tensor.shape[4]):\n","\n","                resized_frame = cv2.resize(tensor[i, j, :, :,k], (0, 0), fx=factor, fy=factor)\n","\n","                resized_frames[i, j, :, :,k] = resized_frame\n","    return resized_frames"]},{"cell_type":"markdown","metadata":{},"source":["Downsize"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:13:30.208971Z","iopub.status.busy":"2023-12-05T19:13:30.207566Z","iopub.status.idle":"2023-12-05T19:13:31.391221Z","shell.execute_reply":"2023-12-05T19:13:31.390102Z","shell.execute_reply.started":"2023-12-05T19:13:30.208907Z"},"trusted":true},"outputs":[],"source":["resized_frames= downsize(0.6,X1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T14:29:56.587034Z","iopub.status.busy":"2023-12-04T14:29:56.586206Z","iopub.status.idle":"2023-12-04T14:30:00.745780Z","shell.execute_reply":"2023-12-04T14:30:00.744726Z","shell.execute_reply.started":"2023-12-04T14:29:56.587003Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","\n","def update(frame):\n","    ax.clear()\n","    mean_polarity_per_location =    resized_frames[:,:,:,:,0][0][frame]\n","\n","    ax.imshow(mean_polarity_per_location, cmap='Greys_r', vmin=0, vmax=1, aspect='auto', origin='lower')\n","    ax.invert_yaxis()\n","    ax.set_title(f'Polarity Heatmap - Interval {frame + 1}')\n","    ax.set_xlabel('X')\n","    ax.set_ylabel('Y')\n","\n","fig, ax = plt.subplots()\n","ani = FuncAnimation(fig, update, frames=30,interval=250, repeat=False)\n","plt.close()\n","\n","\n","from IPython.display import HTML\n","HTML(ani.to_jshtml())"]},{"cell_type":"markdown","metadata":{},"source":["Test set consisting of 3 elements from each word "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:54:58.717911Z","iopub.status.busy":"2023-12-05T14:54:58.717501Z","iopub.status.idle":"2023-12-05T14:54:59.168747Z","shell.execute_reply":"2023-12-05T14:54:59.166939Z","shell.execute_reply.started":"2023-12-05T14:54:58.717883Z"},"trusted":true},"outputs":[],"source":["##TEST SET\n","selected_ranges1 = [slice(3, 32)]\n","for i in range(35, 322, 32):\n","    selected_ranges1.append(slice(i, i + 29))\n","\n","selected_matrix = np.concatenate([resized_frames[range_] for range_ in selected_ranges1], axis=0)\n","\n","selected_ranges = []\n","for i in range(0, 290, 32):\n","    selected_ranges.append(slice(i, i + 3))\n","    \n","TEST=np.concatenate([resized_frames[range_] for range_ in selected_ranges], axis=0)\n","TESTlabels=np.array([i for i in range(10) for _ in range(3)])"]},{"cell_type":"markdown","metadata":{},"source":["Just to be sure, check that the test set does not contain any rows of selected_matrix (our training data) : "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","equal_rows_indices = []\n","\n","for i, test_row in enumerate(TEST):\n","    if any(np.array_equal(test_row, selected_row) for selected_row in selected_matrix):\n","        equal_rows_indices.append(i)\n","\n","print(\"Indices of equal rows:\")\n","print(equal_rows_indices)\n"]},{"cell_type":"markdown","metadata":{},"source":["LSTM model : "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:55:40.189637Z","iopub.status.busy":"2023-12-05T14:55:40.189213Z","iopub.status.idle":"2023-12-05T14:59:10.445262Z","shell.execute_reply":"2023-12-05T14:59:10.444512Z","shell.execute_reply.started":"2023-12-05T14:55:40.189604Z"},"trusted":true},"outputs":[],"source":["\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, TimeDistributed, Flatten,GRU, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n","from keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.optimizers import Adam,SGD\n","from keras.layers import Dropout\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from keras.regularizers import l2\n","from keras.layers import BatchNormalization\n","import tensorflow as tf\n","from keras.losses import SparseCategoricalCrossentropy\n","\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n","\n","\n","custom_optimizer = Adam(learning_rate=0.0001)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(selected_matrix[:,:,:,:,:].reshape(290,30,30*72*3), np.array([i for i in range(10) for _ in range(29)]), test_size=0.2, random_state=35,stratify=np.array([i for i in range(10) for _ in range(29)]))\n","\n","\n","# Create the model\n","model = tf.keras.models.Sequential()\n","model.add(BatchNormalization())\n","model.add(LSTM(units=64, return_sequences=True,dropout=0.1,recurrent_dropout=0.2, input_shape=(30, 30*72*3)))\n","model.add(Dropout(0.5))\n","model.add(LSTM(units=128, return_sequences=True,dropout=0.1,recurrent_dropout=0.3,activation='relu'))\n","#model.add(GRU(units=128, return_sequences=True, kernel_regularizer=l2(0.015),dropout=0.2,recurrent_dropout=0.25,activation='relu'))\n","#model.add(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.015),recurrent_dropout=0.3,activation='relu'))\n","\n","model.add(Flatten())\n","model.add(tf.keras.layers.Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(tf.keras.layers.Dense(128, activation='relu'))\n","#model.add(Dropout(0.5))\n","\n","\n","model.add(Dense(units=10))\n","\n","model.compile(optimizer=custom_optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, epochs=150, batch_size=16, validation_data=(X_test, y_test), shuffle=True, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T14:59:14.718655Z","iopub.status.busy":"2023-12-05T14:59:14.718276Z","iopub.status.idle":"2023-12-05T14:59:21.267627Z","shell.execute_reply":"2023-12-05T14:59:21.266043Z","shell.execute_reply.started":"2023-12-05T14:59:14.718631Z"},"trusted":true},"outputs":[],"source":["Test=TEST[:,:,:,:,:]\n","pred=[]\n","raw_res=[]\n","for i  in range (30) : \n","    result=model.predict(Test[i].reshape(1,30,30*72*3),verbose=None).argmax()\n","    pred.append(result)\n","\n","predtrain=[]\n","raw_res=[]\n","for i  in range (len(X_test)) : \n","    result=model.predict(X_test[i,:,:].reshape(1,30,30*72*3),verbose=None).argmax()\n","    predtrain.append(result)\n","\n","t=(np.array(pred)==TESTlabels).sum()/30\n","t2=(np.array(predtrain)==y_test).sum()/len(X_test)\n","print('validation_accuracy :',t2, '\\n test_accuracy :',t)"]},{"cell_type":"markdown","metadata":{},"source":["CNN/GRU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T10:21:28.770416Z","iopub.status.busy":"2023-12-05T10:21:28.769465Z","iopub.status.idle":"2023-12-05T10:36:01.107532Z","shell.execute_reply":"2023-12-05T10:36:01.105102Z","shell.execute_reply.started":"2023-12-05T10:21:28.770325Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv3D,LSTM, MaxPooling3D, Flatten, Dense,Reshape, BatchNormalization, Dropout,MaxPooling2D, Conv3D, GRU,Activation\n","from tensorflow.keras.optimizers import Adam,SGD\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from keras.regularizers import l2\n","from keras.callbacks import EarlyStopping\n","from keras.regularizers import l2\n","from keras.losses import SparseCategoricalCrossentropy\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n","\n","\n","\n","y=np.array([i for i in range(10) for _ in range(29)])\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(selected_matrix[:,:,:,:,:], y, test_size=0.20, random_state=41,stratify=y)\n","\n","# Build the model\n","custom_optimizer = Adam(learning_rate=0.0001)\n","\n","model = tf.keras.models.Sequential()\n","model.add(BatchNormalization())\n","model.add(Conv3D(64, (5, 2, 2), padding='same', input_shape=(30, 30, 72,3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling3D((2,2, 2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Conv3D(128, (3, 2, 2), padding='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling3D((2, 2, 2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Conv3D(256, (3, 2, 2), padding='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling3D((2, 2, 2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Reshape((3,-1)))\n","model.add(GRU(256, return_sequences=True, recurrent_dropout=0.1, dropout=0))\n","model.add(Dropout(0.5))\n","model.add(Flatten())\n","\n","model.add(Dense(128, activation='relu'))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.5))\n","\n","\n","model.add(tf.keras.layers.Dense(10))\n","\n","# Compile the model\n","model.compile(optimizer=custom_optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=400, batch_size=4, validation_data=(X_test, y_test), callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T05:49:53.156292Z","iopub.status.busy":"2023-12-05T05:49:53.155681Z","iopub.status.idle":"2023-12-05T05:49:59.165339Z","shell.execute_reply":"2023-12-05T05:49:59.164329Z","shell.execute_reply.started":"2023-12-05T05:49:53.156247Z"},"trusted":true},"outputs":[],"source":["Test=TEST[:,:,:,:,[0,1,3]]\n","pred=[]\n","raw_res=[]\n","for i  in range (30) : \n","    result=model.predict(Test[i].reshape(1,30,30,72,3),verbose=None).argmax()\n","    pred.append(result)\n","\n","predtrain=[]\n","raw_res=[]\n","for i  in range (len(X_test)) : \n","    result=model.predict(X_test[i,:,:,:,:].reshape(1,30,30,72,3),verbose=None).argmax()\n","    predtrain.append(result)\n","\n","t=(np.array(pred)==TESTlabels).sum()/30\n","t2=(np.array(predtrain)==y_test).sum()/len(X_test)\n","print('validation_accuracy :',t2, '\\n test_accuracy :',t)"]},{"cell_type":"markdown","metadata":{},"source":["Model seems to work on our own test set, let's learn with all the data available"]},{"cell_type":"markdown","metadata":{},"source":["LSTM with all the data  :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T10:51:28.347862Z","iopub.status.busy":"2023-12-04T10:51:28.347119Z","iopub.status.idle":"2023-12-04T10:55:50.247423Z","shell.execute_reply":"2023-12-04T10:55:50.246503Z","shell.execute_reply.started":"2023-12-04T10:51:28.347829Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, TimeDistributed, Flatten,GRU, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n","from keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.optimizers import Adam,SGD\n","from keras.layers import Dropout\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from keras.regularizers import l2\n","from keras.layers import BatchNormalization\n","import tensorflow as tf\n","from keras.losses import SparseCategoricalCrossentropy\n","\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n","\n","\n","custom_optimizer = Adam(learning_rate=0.0001)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(resized_frames[:,:,:,:,:].reshape(320,30,30*72*3), np.array([i for i in range(10) for _ in range(32)]), test_size=0.2, random_state=35,stratify=np.array([i for i in range(10) for _ in range(32)]))\n","\n","\n","# Create the model\n","model = tf.keras.models.Sequential()\n","model.add(BatchNormalization())\n","model.add(LSTM(units=64, return_sequences=True,dropout=0.1,recurrent_dropout=0.2, input_shape=(30, 30*72*3)))\n","model.add(Dropout(0.5))\n","model.add(LSTM(units=128, return_sequences=True,dropout=0.1,recurrent_dropout=0.3,activation='relu'))\n","#model.add(GRU(units=128, return_sequences=True, kernel_regularizer=l2(0.015),dropout=0.2,recurrent_dropout=0.25,activation='relu'))\n","#model.add(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.015),recurrent_dropout=0.3,activation='relu'))\n","\n","model.add(Flatten())\n","model.add(tf.keras.layers.Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(tf.keras.layers.Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","\n","\n","model.add(Dense(units=10))\n","\n","model.compile(optimizer=custom_optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, epochs=150, batch_size=16, validation_data=(X_test, y_test), shuffle=True, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loss = model.history.history['accuracy']\n","val_loss = model.history.history['val_accuracy']\n","\n","# Plot training loss\n","plt.plot(train_loss, label='Train Accuracy')\n","plt.plot(val_loss, label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy LSTM')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T10:55:52.629527Z","iopub.status.busy":"2023-12-04T10:55:52.628677Z","iopub.status.idle":"2023-12-04T10:55:52.938883Z","shell.execute_reply":"2023-12-04T10:55:52.937886Z","shell.execute_reply.started":"2023-12-04T10:55:52.629490Z"},"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["CNN/GRU with all the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv3D, MaxPooling3D,LSTM, Reshape, Flatten, Dense, BatchNormalization, Dropout,MaxPooling2D, Conv3D, GRU,Activation\n","from tensorflow.keras.optimizers import Adam,SGD\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from keras.regularizers import l2\n","from keras.callbacks import EarlyStopping\n","from keras.regularizers import l2\n","from keras.losses import SparseCategoricalCrossentropy\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n","\n","y=np.array([i for i in range(10) for _ in range(32)])\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(resized_frames[:,:,:,:,:], y, test_size=0.2, random_state=41,stratify=y)\n","\n","# Build the model\n","custom_optimizer = Adam(learning_rate=0.0001)\n","\n","model = tf.keras.models.Sequential()\n","model.add(BatchNormalization())\n","model.add(Conv3D(64, (5, 2, 2), padding='same', input_shape=(30, 30, 72,3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling3D((2,2, 2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Conv3D(128, (3, 2, 2), padding='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling3D((2, 2, 2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Conv3D(256, (3, 2, 2), padding='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling3D((2, 2, 2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Reshape((3,-1)))\n","model.add(GRU(256, return_sequences=True, recurrent_dropout=0.5, dropout=0))\n","model.add(Dropout(0.5))\n","model.add(Flatten())\n","\n","model.add(Dense(128, activation='relu'))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.5))\n","\n","\n","model.add(tf.keras.layers.Dense(10))\n","\n","# Compile the model\n","model.compile(optimizer=custom_optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=400, batch_size=4, validation_data=(X_test, y_test), callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T10:55:55.609381Z","iopub.status.busy":"2023-12-04T10:55:55.608998Z","iopub.status.idle":"2023-12-04T10:55:55.904286Z","shell.execute_reply":"2023-12-04T10:55:55.903418Z","shell.execute_reply.started":"2023-12-04T10:55:55.609348Z"},"trusted":true},"outputs":[],"source":["\n","train_accuracy = history.history['accuracy']\n","val_accuracy = history.history['val_accuracy']\n","\n","plt.plot(train_accuracy, label='Train Accuracy')\n","plt.plot(val_accuracy, label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy CNN3D/LSTM')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T10:56:23.224017Z","iopub.status.busy":"2023-12-04T10:56:23.223304Z","iopub.status.idle":"2023-12-04T10:56:23.275728Z","shell.execute_reply":"2023-12-04T10:56:23.274833Z","shell.execute_reply.started":"2023-12-04T10:56:23.223984Z"},"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T06:53:50.071574Z","iopub.status.busy":"2023-12-05T06:53:50.071074Z","iopub.status.idle":"2023-12-05T06:53:53.301926Z","shell.execute_reply":"2023-12-05T06:53:53.300410Z","shell.execute_reply.started":"2023-12-05T06:53:50.071537Z"},"trusted":true},"outputs":[],"source":["model.save(\"CNNGRUM90.keras\")"]},{"cell_type":"markdown","metadata":{},"source":["Load the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T15:23:29.905443Z","iopub.status.busy":"2023-12-05T15:23:29.904999Z","iopub.status.idle":"2023-12-05T15:23:37.312669Z","shell.execute_reply":"2023-12-05T15:23:37.311338Z","shell.execute_reply.started":"2023-12-05T15:23:29.905409Z"},"trusted":true},"outputs":[],"source":["import os\n","dataframes_list = []\n","directory_path = r'/test10/test10'\n","\n","for file in os.listdir(directory_path):\n","    file_path = os.path.join(directory_path, file)\n","    df = pd.read_csv(file_path)\n","    dataframes_list.append(df) "]},{"cell_type":"markdown","metadata":{},"source":["preprocess it"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T15:23:43.694890Z","iopub.status.busy":"2023-12-05T15:23:43.694448Z","iopub.status.idle":"2023-12-05T15:27:49.112337Z","shell.execute_reply":"2023-12-05T15:27:49.110560Z","shell.execute_reply.started":"2023-12-05T15:23:43.694857Z"},"trusted":true},"outputs":[],"source":["x_range = range(50, 170)\n","y_range = range(15, 65)\n","DF2 = pd.DataFrame([(x, y) for x in x_range for y in y_range], columns=['x', 'y'])\n","all_results = []\n","num_frames=30\n","\n","for df in dataframes_list:\n","    results = []\n","    med=df.loc[:,'time'].median()\n","    if med <600000 :\n","        interval_to_slice=df[(df['time'] < 1200000)]\n","    elif med >2400000 :\n","        interval_to_slice=df[(df['time'])>1800000]\n","    else :\n","        interval_to_slice=df[(df['time']>med-600000) & (df['time']<med+600000)]\n","    \n","    \n","    time_intervals = np.linspace(interval_to_slice['time'].to_numpy()[0], interval_to_slice['time'].to_numpy()[len(interval_to_slice)-1], num=num_frames + 1)\n","    for i in range(len(time_intervals) - 1):   \n","     \n","        start_time, end_time = time_intervals[i], time_intervals[i + 1]\n","        interval_data = interval_to_slice[(interval_to_slice['time'] >= start_time) & (interval_to_slice['time'] < end_time)]\n","        interval_data=interval_data[ (interval_data['x']>=50) & (interval_data['x']<=170) & (interval_data['y']>=15) & (interval_data['y']<=65) ]\n","        last_polarity_per_location =interval_data.groupby(['x', 'y'])['polarity'].sum().reset_index()\n","        last_pol_merged=pd.merge(DF2, last_polarity_per_location, on=['x', 'y'], how='left').fillna(0)\n","\n","        count_polarity_per_location =interval_data.groupby(['x', 'y'])['polarity'].count().reset_index()\n","        count_pol_merged=pd.merge(DF2, count_polarity_per_location, on=['x', 'y'], how='left').fillna(0)\n","\n","        grouped_data = interval_data.groupby(['x', 'y']).agg({'polarity': nb_changes}).reset_index()\n","        DF_merged = pd.merge(DF2, grouped_data, on=['x', 'y'], how='left')\n","        DF_merged['polarity'].fillna(0, inplace=True)\n","\n","        matrix1=last_pol_merged.pivot(index='y', columns='x', values='polarity').values\n","        matrix2=count_pol_merged.pivot(index='y', columns='x', values='polarity').values\n","        matrix3=DF_merged.pivot(index='y', columns='x', values='polarity').values\n","\n","\n","        matrix3D = np.dstack((matrix1,matrix2,matrix3))\n","        results.append(matrix3D)\n","\n","    all_results.append(results)     \n","    print(\"one done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T15:27:56.111097Z","iopub.status.busy":"2023-12-05T15:27:56.110668Z","iopub.status.idle":"2023-12-05T15:27:56.676696Z","shell.execute_reply":"2023-12-05T15:27:56.675224Z","shell.execute_reply.started":"2023-12-05T15:27:56.111067Z"},"trusted":true},"outputs":[],"source":["XX = np.array(all_results)\n","XX1 = XX.reshape(100,30,50,120,3)\n","np.save('newX6r.npy',XX1) ##30 frames, sum and nb of changes\n","XX1= downsize(0.6,XX1)\n","\n","\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["Get the correct labels name instead of just numbers"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:17:20.460059Z","iopub.status.busy":"2023-12-05T19:17:20.459565Z","iopub.status.idle":"2023-12-05T19:17:20.469996Z","shell.execute_reply":"2023-12-05T19:17:20.468392Z","shell.execute_reply.started":"2023-12-05T19:17:20.460020Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","directory_path = r'/train10/train10'\n","\n","correct_labels = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:16:23.975912Z","iopub.status.busy":"2023-12-05T19:16:23.975365Z","iopub.status.idle":"2023-12-05T19:16:26.147057Z","shell.execute_reply":"2023-12-05T19:16:26.145495Z","shell.execute_reply.started":"2023-12-05T19:16:23.975866Z"},"trusted":true},"outputs":[],"source":["XX3=np.load('newX6r.npy')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:16:40.883618Z","iopub.status.busy":"2023-12-05T19:16:40.883092Z","iopub.status.idle":"2023-12-05T19:16:41.158711Z","shell.execute_reply":"2023-12-05T19:16:41.157439Z","shell.execute_reply.started":"2023-12-05T19:16:40.883581Z"},"trusted":true},"outputs":[],"source":["XX3= downsize(0.6,XX3)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:16:19.233108Z","iopub.status.busy":"2023-12-05T19:16:19.232586Z","iopub.status.idle":"2023-12-05T19:16:22.391691Z","shell.execute_reply":"2023-12-05T19:16:22.390428Z","shell.execute_reply.started":"2023-12-05T19:16:19.233066Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","model=tf.keras.models.load_model('CNNGRUM90.keras')"]},{"cell_type":"markdown","metadata":{},"source":["Predict "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:17:32.476622Z","iopub.status.busy":"2023-12-05T19:17:32.475302Z","iopub.status.idle":"2023-12-05T19:17:48.035829Z","shell.execute_reply":"2023-12-05T19:17:48.034800Z","shell.execute_reply.started":"2023-12-05T19:17:32.476554Z"},"trusted":true},"outputs":[],"source":["pred=[]\n","for i  in range (100) : \n","    result=model.predict(XX3[i].reshape(1,30,30,72,3),verbose=None).argmax()\n","    pred.append(correct_labels[result])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:17:58.964413Z","iopub.status.busy":"2023-12-05T19:17:58.963914Z","iopub.status.idle":"2023-12-05T19:17:59.004208Z","shell.execute_reply":"2023-12-05T19:17:59.002893Z","shell.execute_reply.started":"2023-12-05T19:17:58.964370Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","\n","directory_path = r'/test10/test10'\n","\n","file_names = os.listdir(directory_path)\n","\n","numbers = [int(file_name.split('.')[0]) for file_name in file_names]\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:18:01.248874Z","iopub.status.busy":"2023-12-05T19:18:01.248374Z","iopub.status.idle":"2023-12-05T19:18:01.257344Z","shell.execute_reply":"2023-12-05T19:18:01.255826Z","shell.execute_reply.started":"2023-12-05T19:18:01.248822Z"},"trusted":true},"outputs":[],"source":["id = numbers\n","\n","data_list = list(zip(id, pred))\n","import csv\n","file_path = 'sampleSubmission.csv'\n","with open(file_path, 'w', newline='') as csvfile:\n","    csv_writer = csv.writer(csvfile)\n","    csv_writer.writerow(['id', 'label'])  # Header\n","    csv_writer.writerows(data_list)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T19:18:24.757282Z","iopub.status.busy":"2023-12-05T19:18:24.756809Z","iopub.status.idle":"2023-12-05T19:18:24.765402Z","shell.execute_reply":"2023-12-05T19:18:24.763962Z","shell.execute_reply.started":"2023-12-05T19:18:24.757249Z"},"trusted":true},"outputs":[],"source":["id = [i for i in range(100)]\n","\n","data_list = list(zip(id, pred))\n","import csv\n","file_path = 'sampleSubmission.csv'\n","with open(file_path, 'w', newline='') as csvfile:\n","    csv_writer = csv.writer(csvfile)\n","    csv_writer.writerow(['id', 'label'])  # Header\n","    csv_writer.writerows(data_list)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7009041,"sourceId":63982,"sourceType":"competition"},{"datasetId":4093914,"sourceId":7101791,"sourceType":"datasetVersion"},{"datasetId":4115194,"sourceId":7132394,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
