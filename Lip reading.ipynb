{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "dataframes_list = []\n",
    "directory_path = r'c:\\Users\\bounn\\Documents\\train10\\train10'\n",
    "\n",
    "for subdir in os.listdir(directory_path):\n",
    "    subdir_path = os.path.join(directory_path, subdir)\n",
    "\n",
    "    if os.path.isdir(subdir_path):  \n",
    "        word_dataframes = []\n",
    "\n",
    "        for file in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            word_dataframes.append(df)\n",
    "\n",
    "        dataframes_list.append(word_dataframes) # List of list of dataframes, dataframes_list[1] is the list of all data from the csv files in the addition directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nb_changes(polarity_list):\n",
    "    changes = np.diff(polarity_list)\n",
    "    return np.sum(changes != 0)\n",
    "\n",
    "def nb_positive_changes(polarity_list):\n",
    "    if not isinstance(polarity_list, list):\n",
    "        return 0  # Return 0 if the input is not a list\n",
    "    \n",
    "    positive_changes = 0\n",
    "    \n",
    "    for i in range(1, len(polarity_list)):\n",
    "        if polarity_list[i] == 1 and polarity_list[i-1] == 0:\n",
    "            positive_changes += 1\n",
    "    \n",
    "    return positive_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice into frames and extract features from the data we have before normalizing them.\n",
    "We extract the number of changes each pixel had during a timeframe, the number of positive changes it had (going from 0 to 1) aswell as the sum of the polarities captured.\n",
    "We then store all of these informations into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = range(0, 224)\n",
    "y_range = range(0, 90)\n",
    "DF2 = pd.DataFrame([(x, y) for x in x_range for y in y_range], columns=['x', 'y'])\n",
    "all_results = []\n",
    "num_frames=30\n",
    "for list_of_df in dataframes_list:\n",
    "    list_results = []  # List of results for each time interval\n",
    "\n",
    "    # Loop over each dataframe in the inner list\n",
    "    for df in list_of_df:\n",
    "        results = []\n",
    "        med=df.loc[:,'time'].median()\n",
    "        if med <600000 :\n",
    "            interval_to_slice=df[(df['time'] < 1200000)]\n",
    "        elif med >2400000 :\n",
    "            interval_to_slice=df[(df['time'])>1800000]\n",
    "        else :\n",
    "            interval_to_slice=df[(df['time']>med-600000) & (df['time']<med+600000)]\n",
    "        \n",
    "        \n",
    "        time_intervals = np.linspace(interval_to_slice['time'].to_numpy()[0], interval_to_slice['time'].to_numpy()[len(interval_to_slice)-1], num=num_frames + 1)\n",
    "        for i in range(len(time_intervals) - 1):    \n",
    "            start_time, end_time = time_intervals[i], time_intervals[i + 1]\n",
    "            interval_data = interval_to_slice[(interval_to_slice['time'] >= start_time) & (interval_to_slice['time'] < end_time)]\n",
    "            last_polarity_per_location =interval_data.groupby(['x', 'y'])['polarity'].sum().reset_index()\n",
    "            last_pol_merged=pd.merge(DF2, last_polarity_per_location, on=['x', 'y'], how='left').fillna(0)\n",
    "            scaler = MinMaxScaler()\n",
    "            last_pol_merged['polarity']=scaler.fit_transform(last_pol_merged['polarity'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "\n",
    "            grouped_data = interval_data.groupby(['x', 'y']).agg({'polarity': nb_changes}).reset_index()\n",
    "            DF_merged = pd.merge(DF2, grouped_data, on=['x', 'y'], how='left')\n",
    "            DF_merged['polarity'].fillna(0, inplace=True)\n",
    "            DF_merged['polarity'] = scaler.fit_transform(DF_merged[['polarity']])\n",
    "            \n",
    "            grouped_data2=pd.DataFrame(interval_data.groupby(['x', 'y']).agg({'polarity': list}).reset_index())\n",
    "            DF_merged2 = pd.merge(DF2, grouped_data2, on=['x', 'y'], how='left')\n",
    "            DF_merged2['polarity'].fillna(0, inplace=True)\n",
    "            DF_merged2['polarity'] = scaler.fit_transform(DF_merged2.apply(lambda row: nb_positive_changes(row['polarity']), axis=1).to_numpy().reshape(-1, 1))\n",
    "            \n",
    "\n",
    "\n",
    "            matrix1=last_pol_merged.pivot(index='y', columns='x', values='polarity').values \n",
    "            matrix2=DF_merged2.pivot(index='y', columns='x', values='polarity').values ##nb of changes\n",
    "            matrix3=DF_merged.pivot(index='y', columns='x', values='polarity').values ##nb of changes\n",
    "            \n",
    "            \n",
    "\n",
    "            matrix3D = np.dstack((matrix1,matrix2,matrix3))\n",
    "            results.append(matrix3D)\n",
    "        list_results.append(results)\n",
    "    all_results.append(list_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store all the information in a tensor of shape 320,30,90,224,3 and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flattened_matrices = [matrix for list_of_results in all_results for results in list_of_results for matrix in results]\n",
    "\n",
    "X = np.array(flattened_matrices)\n",
    "X1 = X.reshape(320,30,90,224,3)\n",
    "np.save('newX5.npy',X) ##30 frames, sum and nb of changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.load('newX5.npy')\n",
    "X1 = X.reshape(320,30,90,224,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the grid to keep only the lips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = X1[:,:, 20:70,55:170:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 3D GRU architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout,MaxPooling2D, Conv3D, GRU, Reshape\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3[:,:,:,:,:], np.array([i for i in range(10) for _ in range(32)]), test_size=0.20, random_state=41)\n",
    "\n",
    "# Build the model\n",
    "custom_optimizer = Adam(learning_rate=0.0001)\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv3D(16, (2,2,2),padding='same', kernel_regularizer=l2(0.015), input_shape=(30,50,115,3)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Conv3D(32, (2,2,2),padding='same',kernel_regularizer=l2(0.015)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPooling3D((2,2,2)))\n",
    "\n",
    "model.add(Reshape((15,-1)))\n",
    "model.add(GRU(units=16, return_sequences=True, kernel_regularizer=l2(0.015),recurrent_dropout=0.1,activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "'''model.add(tf.keras.layers.Dense(128,kernel_regularizer=l2(0.01)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))'''\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 29s 652ms/step - loss: 12.1570 - accuracy: 0.1055 - val_loss: 10.8159 - val_accuracy: 0.0781\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 23s 741ms/step - loss: 9.8086 - accuracy: 0.2227 - val_loss: 8.8630 - val_accuracy: 0.1875\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 18s 561ms/step - loss: 8.1606 - accuracy: 0.2383 - val_loss: 7.5030 - val_accuracy: 0.1406\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 21s 651ms/step - loss: 7.0049 - accuracy: 0.2773 - val_loss: 6.5587 - val_accuracy: 0.2031\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 19s 576ms/step - loss: 6.1739 - accuracy: 0.2969 - val_loss: 5.8694 - val_accuracy: 0.2969\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 19s 581ms/step - loss: 5.5275 - accuracy: 0.3594 - val_loss: 5.3246 - val_accuracy: 0.2812\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 19s 601ms/step - loss: 4.9331 - accuracy: 0.4844 - val_loss: 4.8035 - val_accuracy: 0.4219\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 19s 600ms/step - loss: 4.4282 - accuracy: 0.5469 - val_loss: 4.5102 - val_accuracy: 0.4062\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 19s 604ms/step - loss: 4.0662 - accuracy: 0.5547 - val_loss: 4.2850 - val_accuracy: 0.4844\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 18s 571ms/step - loss: 3.7363 - accuracy: 0.6406 - val_loss: 4.2986 - val_accuracy: 0.3906\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 18s 573ms/step - loss: 3.4670 - accuracy: 0.7344 - val_loss: 3.9958 - val_accuracy: 0.4688\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 19s 586ms/step - loss: 3.2820 - accuracy: 0.7539 - val_loss: 3.9340 - val_accuracy: 0.5781\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 20s 642ms/step - loss: 3.0707 - accuracy: 0.8203 - val_loss: 3.9438 - val_accuracy: 0.5312\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 19s 586ms/step - loss: 2.9446 - accuracy: 0.8164 - val_loss: 4.0390 - val_accuracy: 0.5156\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 17s 525ms/step - loss: 2.7477 - accuracy: 0.8789 - val_loss: 3.9716 - val_accuracy: 0.5625\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 15s 479ms/step - loss: 2.6156 - accuracy: 0.9102 - val_loss: 3.9957 - val_accuracy: 0.5312\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 16s 501ms/step - loss: 2.5146 - accuracy: 0.9141 - val_loss: 3.9410 - val_accuracy: 0.4844\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 17s 525ms/step - loss: 2.4394 - accuracy: 0.9141 - val_loss: 3.8490 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 16s 502ms/step - loss: 2.3407 - accuracy: 0.9531 - val_loss: 3.9924 - val_accuracy: 0.5156\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 16s 505ms/step - loss: 2.2362 - accuracy: 0.9609 - val_loss: 3.8257 - val_accuracy: 0.4531\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 16s 502ms/step - loss: 2.2571 - accuracy: 0.9336 - val_loss: 3.7821 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 17s 534ms/step - loss: 2.1562 - accuracy: 0.9531 - val_loss: 3.8653 - val_accuracy: 0.5156\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 16s 512ms/step - loss: 2.0915 - accuracy: 0.9570 - val_loss: 3.7831 - val_accuracy: 0.5312\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 17s 537ms/step - loss: 2.0512 - accuracy: 0.9492 - val_loss: 3.7699 - val_accuracy: 0.5156\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 17s 518ms/step - loss: 1.9734 - accuracy: 0.9766 - val_loss: 3.7867 - val_accuracy: 0.5156\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 15s 482ms/step - loss: 1.9666 - accuracy: 0.9648 - val_loss: 3.8899 - val_accuracy: 0.5469\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 16s 500ms/step - loss: 1.8933 - accuracy: 0.9727 - val_loss: 3.6983 - val_accuracy: 0.5312\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 16s 499ms/step - loss: 1.8757 - accuracy: 0.9648 - val_loss: 3.5962 - val_accuracy: 0.5156\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 16s 490ms/step - loss: 1.8206 - accuracy: 0.9766 - val_loss: 3.6190 - val_accuracy: 0.5156\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 16s 511ms/step - loss: 1.7391 - accuracy: 0.9844 - val_loss: 3.9119 - val_accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 18s 559ms/step - loss: 1.6903 - accuracy: 0.9844 - val_loss: 3.8663 - val_accuracy: 0.5312\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 17s 527ms/step - loss: 1.6736 - accuracy: 0.9883 - val_loss: 3.7403 - val_accuracy: 0.4688\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 1.6164 - accuracy: 0.9922 - val_loss: 3.8433 - val_accuracy: 0.5625\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 17s 517ms/step - loss: 1.5934 - accuracy: 0.9766 - val_loss: 3.7760 - val_accuracy: 0.5469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x260f53e8640>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Flatten,GRU, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.00009)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3[:,:,:,:,:].reshape(320,30,50*115*3), np.array([i for i in range(10) for _ in range(32)]), test_size=0.20, random_state=41)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "model.add(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.016),dropout=0.1,recurrent_dropout=0.4, input_shape=(30, 50*115*3)))\n",
    "model.add(GRU(units=128, return_sequences=True, kernel_regularizer=l2(0.016),activation='relu',dropout=0.2,recurrent_dropout=0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.016)))\n",
    "model.add(Dropout(0.06))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.016)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=8, validation_data=(X_test, y_test), shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##resized crop : same result\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Flatten,GRU, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.00008)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3[:,:,:,:,:].reshape(320,30,50*115*2), np.array([i for i in range(10) for _ in range(32)]), test_size=0.2, random_state=35)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "model.add(GRU(units=32, return_sequences=True, kernel_regularizer=l2(0.015),dropout=0.015,recurrent_dropout=0.25, input_shape=(30, 50*115*2)))\n",
    "\n",
    "model.add(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.015),dropout=0.015,recurrent_dropout=0.25,activation='relu'))\n",
    "\n",
    "#model.add(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.015),recurrent_dropout=0.3,activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), shuffle=True, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
